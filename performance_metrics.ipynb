{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "performance_metrics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeshavGulati/Flexbox-ch-04/blob/master/performance_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubfAQY963xXt"
      },
      "source": [
        "# Measuring Model Performance in Python\n",
        "## Philip Mathew (AI Lead)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7TNpCcd565F"
      },
      "source": [
        "First, let's just quickly train a model on a dataset. We'll be using MNIST for this specific notebook, and I'm just going to use a simple ANN for classification. Don't sweat these details much, they're not the focus of this problem.\n",
        "\n",
        "What is important however, is the modifications I'm making. MNIST is a very balanced dataset, but I'm going to be making the training data unbalanced in order to demonstrate the effects of imbalance on accuracy. To do so, I'm going to make it so that the number of examples in the training set tapers off as the nominal value of the label increases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeiEg9Kd4DRn"
      },
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train.reshape(x_train.shape[0], 784), x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "class_prop = list(reversed(np.linspace(0, 1, num=10)))\n",
        "\n",
        "tmp = []\n",
        "labels = []\n",
        "for i in range(10):\n",
        "  # For each class, reduce the total number of examples form that class to\n",
        "  # fit the desired proportions\n",
        "  class_vecs = x_train[np.where(y_train == i)]\n",
        "  class_vecs = class_vecs[0:max(5, int(5000 * class_prop[i]))]\n",
        "  for vec in class_vecs:\n",
        "    tmp.append(vec)\n",
        "    labels.append(i)\n",
        "\n",
        "# Shuffles training and testing set concurrently\n",
        "c = list(zip(tmp, labels))\n",
        "random.shuffle(c)\n",
        "a, b = zip(*c)\n",
        "x_train = np.asarray(a)\n",
        "y_train = np.asarray(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "classes = list(range(10))\n",
        "freqs1, freqs2 = np.bincount(y_train), np.bincount(y_test)\n",
        "freqs1 = [freqs1[i] if i < len(freqs1) else 0 for i in classes]\n",
        "freqs2 = [freqs2[i] if i < len(freqs2) else 0 for i in classes]\n",
        "\n",
        "ax1.bar(classes, freqs1)\n",
        "ax1.set_xticks(classes)\n",
        "ax1.set_title('Distribution of Training Data')\n",
        "\n",
        "ax2.bar(classes, freqs2)\n",
        "ax2.set_xticks(classes)\n",
        "ax2.set_title('Distribution of Test Data');"
      ],
      "metadata": {
        "id": "UgELBZtFHTfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWKPvb5yXjsH"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nEiJ7dU8PW6"
      },
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5D81VrU9vhJ"
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVdxF3dV4G71"
      },
      "source": [
        "Now, the most widely used library for measuring performance is naturally ```sklearn```, speacifically the ```sklearn.metrics``` module. I encourage all of you to read through the functions in there to understand the various metrics used to measure models.\n",
        "\n",
        "In practice, however, theres' one specific method that's often used: ```sklearn.metrics.classification_report()```. In short, it computes all of the easy metrics (like accuracy, [recall, precision](https://en.wikipedia.org/wiki/Precision_and_recall), etc). In binary classification problems it computes extra metrics like AUC, however since MNIST is a multiclass problem we don't get to see this capability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pURBa51q98yF"
      },
      "source": [
        "from sklearn.metrics import *\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBNMft-aYW5h"
      },
      "source": [
        "Now, as we can see, this model has an ~85% accuracy rating (Note: This will most certainly be different for you, but shouldn't be more than like 5% off), which is honestly atrocious as far as MNIST goes. In general, however, it's not a bad score. With that said, let's draw up the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBsKbu4xeO33"
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confmat(true_labels, pred_labels):\n",
        "    \"\"\"\n",
        "    Plots a confusion matrix from given data\n",
        "    \"\"\"\n",
        "    fig2, ax = plt.subplots(1, 1, num=2, figsize=(10, 10))\n",
        "\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # normalize the confusion matrix\n",
        "    for pair in np.argwhere(np.isnan(cm_norm)):\n",
        "        cm_norm[pair[0]][pair[1]] = 0\n",
        "\n",
        "    annot = np.zeros_like(cm, dtype=object)\n",
        "    for i in range(annot.shape[0]):  # Creates an annotation array for the heatmap\n",
        "        for j in range(annot.shape[1]):\n",
        "            annot[i][j] = f'{cm[i][j]}\\n{round(cm_norm[i][j] * 100, ndigits=3)}%'\n",
        "\n",
        "    ax = sns.heatmap(cm_norm, annot=annot, fmt='', cbar=True, cmap=plt.cm.magma, vmin=0, ax=ax) # plot the confusion matrix\n",
        "\n",
        "    ax.set(xlabel='Predicted Label', ylabel='Actual Label')\n",
        "\n",
        "    fig2.tight_layout()\n",
        "\n",
        "plot_confmat(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdM5USHRfhme"
      },
      "source": [
        "(sidenote: feel free to steal this function, I'll take hot chocolate as royalties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q14jOe7fgI3q"
      },
      "source": [
        "Now the confusion matrix demonstrates the problems here. As we can see, the model downright terrible accuracy when predicting classes 8 and 9. As such, in order to get a better model, we'd need to find a way to fix this. Notice that we would not have seen this if we took the overall accuracy score at face value."
      ]
    }
  ]
}